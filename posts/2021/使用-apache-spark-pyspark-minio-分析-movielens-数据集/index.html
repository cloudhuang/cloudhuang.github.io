<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集 | TechNotes</title><meta name=keywords content="publish,大数据,Spark,PySpark"><meta name=description content="MinIOMinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。MinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。MinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。## Apache Spark / PySparkApache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。Apache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。## Jupyter NotebookJupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。## MovieLens 数据集GroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。数据集下载地址： http://files.grouplens.org/datasets/movielens/ml-latest.zip下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。## 准备环境这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理version: '3.7'# Settings and configurations that are common for all containersx-minio-common: &minio-commonimage: minio/miniocommand: server --console-address &#34;:9001&#34; http://minio{1."><meta name=author content="TechNotes"><link rel=canonical href=https://technotes.guru/posts/2021/%E4%BD%BF%E7%94%A8-apache-spark-pyspark-minio-%E5%88%86%E6%9E%90-movielens-%E6%95%B0%E6%8D%AE%E9%9B%86/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.196211a537485b3a9509ba7a77d2de5700c6d26cb8d6c95b483ea1ec528a1d43.css integrity="sha256-GWIRpTdIWzqVCbp6d9LeVwDG0my41slbSD6h7FKKHUM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://technotes.guru/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://technotes.guru/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://technotes.guru/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://technotes.guru/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://technotes.guru/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.76.5"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集"><meta property="og:description" content="MinIOMinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。MinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。MinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。## Apache Spark / PySparkApache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。Apache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。## Jupyter NotebookJupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。## MovieLens 数据集GroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。数据集下载地址： http://files.grouplens.org/datasets/movielens/ml-latest.zip下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。## 准备环境这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理version: '3.7'# Settings and configurations that are common for all containersx-minio-common: &minio-commonimage: minio/miniocommand: server --console-address &#34;:9001&#34; http://minio{1."><meta property="og:type" content="article"><meta property="og:url" content="https://technotes.guru/posts/2021/%E4%BD%BF%E7%94%A8-apache-spark-pyspark-minio-%E5%88%86%E6%9E%90-movielens-%E6%95%B0%E6%8D%AE%E9%9B%86/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-29T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-29T00:00:00+00:00"><meta property="og:site_name" content="TechNotes"><meta name=twitter:card content="summary"><meta name=twitter:title content="使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集"><meta name=twitter:description content="MinIOMinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。MinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。MinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。## Apache Spark / PySparkApache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。Apache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。## Jupyter NotebookJupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。## MovieLens 数据集GroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。数据集下载地址： http://files.grouplens.org/datasets/movielens/ml-latest.zip下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。## 准备环境这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理version: '3.7'# Settings and configurations that are common for all containersx-minio-common: &minio-commonimage: minio/miniocommand: server --console-address &#34;:9001&#34; http://minio{1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://technotes.guru/posts/"},{"@type":"ListItem","position":2,"name":"使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集","item":"https://technotes.guru/posts/2021/%E4%BD%BF%E7%94%A8-apache-spark-pyspark-minio-%E5%88%86%E6%9E%90-movielens-%E6%95%B0%E6%8D%AE%E9%9B%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集","name":"使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集","description":"MinIO\rMinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。\rMinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。\rMinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。\r## Apache Spark / PySpark\rApache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。\rApache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。\r## Jupyter Notebook\rJupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。\r## MovieLens 数据集\rGroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。\r数据集下载地址： http://files.grouplens.org/datasets/movielens/ml-latest.zip\r下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。\r## 准备环境\r这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理\r\rversion: '3.7'\r# Settings and configurations that are common for all containers\rx-minio-common: \u0026amp;minio-common\rimage: minio/minio\rcommand: server --console-address \u0026quot;:9001\u0026quot; http://minio{1.","keywords":["publish","大数据","Spark","PySpark"],"articleBody":"MinIO\rMinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。\rMinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。\rMinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。\r## Apache Spark / PySpark\rApache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。\rApache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。\r## Jupyter Notebook\rJupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。\r## MovieLens 数据集\rGroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。\r数据集下载地址： http://files.grouplens.org/datasets/movielens/ml-latest.zip\r下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。\r## 准备环境\r这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理\r\rversion: '3.7'\r# Settings and configurations that are common for all containers\rx-minio-common: \u0026minio-common\rimage: minio/minio\rcommand: server --console-address \":9001\" http://minio{1...4}/data{1...2}\rexpose:\r- \"9000\"\r- \"9001\"\renvironment:\rMINIO_ROOT_USER: minio\rMINIO_ROOT_PASSWORD: minio123\rhealthcheck:\rtest: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\rinterval: 30s\rtimeout: 20s\rretries: 3\r# starts 4 docker containers running minio server instances.\r# using nginx reverse proxy, load balancing, you can access\r# it through port 9000.\rservices:\rminio1:\r然后通过docker compose up -d启动集群. 在这个 docker compose 文件中同时化部署了PostgreSQL数据库以及PostgreSQL Admin 管理UI。\r### MinIO GUI\r通过上面的 docker compose 命令启动了启动了4个实例的MinIO集群，并通过Nginx进行反向代理, 可以通过 http://localhost:9000 来访问 MinIO 的控制台。\r### Jupyter Notebook\r通过 http://localhost:8888 访问 Jupyter Notebook。\r由于访问Nupyter Dashboard首先需要一个token，这里首先需要从 docker logs 中获取该 token\r\rdocker logs $(docker ps | grep jupyter_notebook | awk '{print $NF}')\r\r从 log 中可以看到 ?token=93bc05d6549e689c3409a0ac60b58883c13236aa94245306 的内容，就可以使用这个 token 来登录了:\r### PostgreSQL Admin\r通过 http://localhost:5050 来访问 PostgreSQL admin dashboard, 通过 “Add New Server” 将 PostgreSQL 添加进来，然后就可以通过 PostgreSQL Admin 来管理数据库了。 ## 导入测试数据集\r下载上面的电影评分数据集，并在 MinIO 中创建 bucket1 Bucket，并将解压后的 csv 文件上传到该 bucket 中。\r总共数据大小在1GB左右。\r## 创建 Jupyter Notebook\r直接通过JupyterLab创建Notebook, 然后直接在notebook中直接编写python代码来调用Spark进行数据的分析操作。\r比如这里调用 Spark 来计算从1加到100的值：\r在 docker compose的 yaml 配置中， notebook 通过 environment 属性配置了 PYSPARK_SUBMIT_ARGS 参数，初次运行 Spark 的时候，会检查并下载指定的 packages。\r这里主要使用了AWS的 JAVA SDK，通过Amazon S3 API 同 MinIO 进行通信。\ryaml\renvironment:\r- PYSPARK_SUBMIT_ARGS=--packages com.amazonaws:aws-java-sdk:1.12.95,org.apache.hadoop:hadoop-client:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.95,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell\r\r相应的Spark的Job可以通过 http://localhost:4040 页面查看:\r## 分析 MovieLens 数据集\r下面就通过Spark来分析MovidLens的数据集。\r主要是从MinIO读取数据集中的CSV文件，注册成为table，通过SQL查询出top 100的电影，然后将分析结果保存到MinIO以及PostgreSQL数据库。\r### 初始化 SparkSession\r在读取CSV文件之前，首先需要创建 SparkSession 的实例 spark，由于Spark主要是内存计算，这里通过spark.driver.memory配置参数配置两个内存，否则运行过程中会产生OOM的问题。\r\rfrom pyspark.sql import SparkSession\rspark = SparkSession.builder.config(\"spark.driver.memory\", \"5g\").getOrCreate()\r\r### 配置S3/MinIO连接信息\rpython\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minio123\")\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://192.168.0.9:9000\")\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\rspark.sparkContext._jsc\\\r.hadoopConfiguration().set(\"fs.s3a.multipart.size\", \"104857600\")\r\r### 读取 MinIO CSV 文件\rpython\rratings = spark.read\\\r.option(\"header\", \"true\")\\\r.option(\"inferSchema\", \"true\")\\\r.csv(\"s3a://bucket1/ratings.csv\")\rratings.createOrReplaceTempView(\"ratings\")\rratings.show()\r\rCSV中的内容则以表格的形式显示如下：\r\r+------+-------+------+----------+\r|userId|movieId|rating| timestamp|\r+------+-------+------+----------+\r| 1| 307| 3.5|1256677221|\r| 1| 481| 3.5|1256677456|\r| 1| 1091| 1.5|1256677471|\r| 1| 1257| 4.5|1256677460|\r| 1| 1449| 4.5|1256677264|\r| 1| 1590| 2.5|1256677236|\r| 1| 1591| 1.5|1256677475|\r| 1| 2134| 4.5|1256677464|\r| 1| 2478| 4.0|1256677239|\r| 1| 2840| 3.0|1256677500|\r| 1| 2986| 2.5|1256677496|\r| 1| 3020| 4.0|1256677260|\r| 1| 3424| 4.5|1256677444|\r| 1| 3698| 3.5|1256677243|\r| 1| 3826| 2.0|1256677210|\r| 1| 3893| 3.5|1256677486|\r| 2| 170| 3.5|1192913581|\r| 2| 849| 3.5|1192913537|\r| 2| 1186| 3.5|1192913611|\r| 2| 1235| 3.0|1192913585|\r+------+-------+------+----------+\ronly showing top 20 rows\r\r再次读取moviesCSV\rpython\rmovies = spark.read\\\r.option(\"header\", \"true\")\\\r.option(\"inferSchema\", \"true\")\\\r.csv(\"s3a://bucket1/movies.csv\")\rmovies.registerTempTable(\"movies\")\rmovies.show()\r+-------+--------------------+--------------------+\r|movieId| title| genres|\r+-------+--------------------+--------------------+\r| 1| Toy Story (1995)|Adventure|Animati...|\r| 2| Jumanji (1995)|Adventure|Childre...|\r| 3|Grumpier Old Men ...| Comedy|Romance|\r| 4|Waiting to Exhale...|Comedy|Drama|Romance|\r| 5|Father of the Bri...| Comedy|\r| 6| Heat (1995)|Action|Crime|Thri...|\r| 7| Sabrina (1995)| Comedy|Romance|\r| 8| Tom and Huck (1995)| Adventure|Children|\r| 9| Sudden Death (1995)| Action|\r| 10| GoldenEye (1995)|Action|Adventure|...|\r| 11|American Presiden...|Comedy|Drama|Romance|\r| 12|Dracula: Dead and...| Comedy|Horror|\r| 13| Balto (1995)|Adventure|Animati...|\r| 14| Nixon (1995)| Drama|\r| 15|Cutthroat Island ...|Action|Adventure|...|\r| 16| Casino (1995)| Crime|Drama|\r| 17|Sense and Sensibi...| Drama|Romance|\r| 18| Four Rooms (1995)| Comedy|\r| 19|Ace Ventura: When...| Comedy|\r| 20| Money Train (1995)|Action|Comedy|Cri...|\r+-------+--------------------+--------------------+\ronly showing top 20 rows\r\r### 计算 top 100\rpython\rtop_100_movies = spark.sql(\"\"\"\rSELECT title, AVG(rating) as avg_rating\rFROM movies m\rLEFT JOIN ratings r ON m.movieId = r.movieID\rGROUP BY title\rHAVING COUNT(*)  100\rORDER BY avg_rating DESC\rLIMIT 100\r\"\"\")\rtop_100_movies.show()\r# Spark Processing\r+--------------------+------------------+\r| title| avg_rating|\r+--------------------+------------------+\r|Planet Earth II (...|4.4865181711606095|\r| Planet Earth (2006)| 4.458092485549133|\r|Shawshank Redempt...| 4.424188001918387|\r|Band of Brothers ...| 4.399898373983739|\r|Black Mirror: Whi...| 4.350558659217877|\r| Cosmos| 4.343949044585988|\r|The Godfather Tri...| 4.339667458432304|\r|Godfather, The (1...| 4.332892749244713|\r|Usual Suspects, T...| 4.291958829205532|\r| Black Mirror| 4.263888888888889|\r|Godfather: Part I...|4.2630353697749195|\r|Last Year's Snow ...| 4.261904761904762|\r|Schindler's List ...| 4.257501817775044|\r|Seven Samurai (Sh...|4.2541157909178215|\r|Over the Garden W...| 4.244031830238727|\r|Sherlock - A Stud...| 4.23943661971831|\r| 12 Angry Men (1957)| 4.237075455914338|\r|Blue Planet II (2...| 4.236389684813753|\r| Rear Window (1954)| 4.230798598634567|\r| Fight Club (1999)| 4.230663235786717|\r+--------------------+------------------+\ronly showing top 20 rows\r\r### 保存 top 100 到 MinIO\rpython\rtop_100_movies.write.parquet(\"s3a://bucket1/results/top_100_movies\")\r\r当运行结束后，可以通过MinIO控制台查看，对应的top 100的数据，已经保存到了MinIO中了。\r### 从MinIO读取Parquet\rpython\rspark.read.parquet(\"s3a://bucket1/results/top_100_movies\").show()\r+--------------------+------------------+\r| title| avg_rating|\r+--------------------+------------------+\r|Planet Earth II (...|4.4865181711606095|\r| Planet Earth (2006)| 4.458092485549133|\r|Shawshank Redempt...| 4.424188001918387|\r|Band of Brothers ...| 4.399898373983739|\r|Black Mirror: Whi...| 4.350558659217877|\r| Cosmos| 4.343949044585988|\r|The Godfather Tri...| 4.339667458432304|\r|Godfather, The (1...| 4.332892749244713|\r|Usual Suspects, T...| 4.291958829205532|\r| Black Mirror| 4.263888888888889|\r|Godfather: Part I...|4.2630353697749195|\r|Last Year's Snow ...| 4.261904761904762|\r|Schindler's List ...| 4.257501817775044|\r|Seven Samurai (Sh...|4.2541157909178215|\r|Over the Garden W...| 4.244031830238727|\r|Sherlock - A Stud...| 4.23943661971831|\r| 12 Angry Men (1957)| 4.237075455914338|\r|Blue Planet II (2...| 4.236389684813753|\r| Rear Window (1954)| 4.230798598634567|\r| Fight Club (1999)| 4.230663235786717|\r+--------------------+------------------+\ronly showing top 20 rows\r\r### 保存数据到PostgreSQL数据库\r上面将分析的数据保存到了MinIO文件系统，下面则将 top 100 的电影数据保存到 PostgreSQL 数据库。\r#### 安装pipy包\r为了访问PostgreSQL数据库，首先需要安装相应的psycopg2-binary和sqlalchemypython包。\rpython\rpip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com psycopg2-binary sqlalchemy\r\r#### 保存到数据库\rpython\rimport psycopg2\rimport pandas as pd\rfrom sqlalchemy import create_engine\rtop_100_df = top_100_movies.toPandas()\r# Create SQLAlchemy engine\rengine = create_engine(\"postgresql+psycopg2://root:root@192.168.0.9:5432/test_db?client_encoding=utf8\")\r# Save result to the database via engine\rtop_100_df.to_sql('test_table', engine, index=False, if_exists='replace')\r\r执行完成之后，就可以在PGAdmin中查看结果了。\r#### 读取PostgreSQL数据\r上面是保存到数据库，同样的也可以读取数据库中的表数据，然后加载到Spark中进行分析\rpython\rimport psycopg2\rimport pandas as pd\rfrom pyspark.sql import SparkSession\rfrom sqlalchemy import create_engine\rengine = create_engine(\"postgresql+psycopg2://admin:admin@192.168.0.9:5432/test_db?client_encoding=utf8\")\rpdf = pd.read_sql('select index.md from test_table', engine)\r# Convert Pandas dataframe to spark DataFrame\rdf = spark.createDataFrame(pdf)\rprint(df.schema)\rdf.show()\rStructType(List(StructField(title,StringType,true),StructField(avg_rating,DoubleType,true)))\r+--------------------+------------------+\r| title| avg_rating|\r+--------------------+------------------+\r|Planet Earth II (...|4.4865181711606095|\r| Planet Earth (2006)| 4.458092485549133|\r|Shawshank Redempt...| 4.424188001918387|\r|Band of Brothers ...| 4.399898373983739|\r|Black Mirror: Whi...| 4.350558659217877|\r| Cosmos| 4.343949044585988|\r|The Godfather Tri...| 4.339667458432304|\r|Godfather, The (1...| 4.332892749244713|\r|Usual Suspects, T...| 4.291958829205532|\r| Black Mirror| 4.263888888888889|\r|Godfather: Part I...|4.2630353697749195|\r|Last Year's Snow ...| 4.261904761904762|\r|Schindler's List ...| 4.257501817775044|\r|Seven Samurai (Sh...|4.2541157909178215|\r|Over the Garden W...| 4.244031830238727|\r|Sherlock - A Stud...| 4.23943661971831|\r| 12 Angry Men (1957)| 4.237075455914338|\r|Blue Planet II (2...| 4.236389684813753|\r| Rear Window (1954)| 4.230798598634567|\r| Fight Club (1999)| 4.230663235786717|\r+--------------------+------------------+\ronly showing top 20 rows\r\r## 总结\r上面显示了通过 Apache Spark/PySpark, MinIO来分析MovieLens数据集，得益于Docker, Jupyter Docker Stacks等基础工具，从构建环境，到使用Jupyter笔记本、Python、Spark和PySpark开始学习和执行数据分析相当的容易，并且还可以在堆栈中添加额外的容器，如MySQL、MongoDB、RabbitMQ、Apache Kafka和Apache Cassandra。构建一个更为完备的分析平台。 ","wordCount":"1087","inLanguage":"en","datePublished":"2021-10-29T00:00:00Z","dateModified":"2021-10-29T00:00:00Z","author":{"@type":"Person","name":"TechNotes"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://technotes.guru/posts/2021/%E4%BD%BF%E7%94%A8-apache-spark-pyspark-minio-%E5%88%86%E6%9E%90-movielens-%E6%95%B0%E6%8D%AE%E9%9B%86/"},"publisher":{"@type":"Organization","name":"TechNotes","logo":{"@type":"ImageObject","url":"https://technotes.guru/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: #1d1e20;--entry: #2e2e33;--primary: rgba(255, 255, 255, 0.84);--secondary: rgba(255, 255, 255, 0.56);--tertiary: rgba(255, 255, 255, 0.16);--content: rgba(255, 255, 255, 0.74);--hljs-bg: #2e2e33;--code-bg: #37383e;--border: #333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://technotes.guru/ accesskey=h title="TechNotes (Alt + H)">TechNotes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://technotes.guru/categories/ title=categories><span>categories</span></a></li><li><a href=https://technotes.guru/tags/ title=tags><span>tags</span></a></li><li><a href=https://technotes.guru/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://technotes.guru/>Home</a>&nbsp;»&nbsp;<a href=https://technotes.guru/posts/>Posts</a></div><h1 class=post-title>使用 Apache Spark, PySpark MinIO 分析 MovieLens 数据集</h1><div class=post-meta>2021-10-10&nbsp;·&nbsp;6 min&nbsp;·&nbsp;TechNotes</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner></div></details></div><div class=post-content><h2 id=minio-minio是一个用golang开发的基于gnu-affero-general-public-license-v30开源协议的高性能对象存储服务--minio兼容亚马逊s3云存储服务接口非常适合于存储大容量非结构化的数据例如图片视频日志文件备份数据和容器虚拟机镜像等它可以部署在企业的内部环境中为机器学习分析和应用数据工作负载建立高性能的存储基础设施--minio用作云原生应用程序的主要存储解决方案可以与kubernetes相结合成为hadoop生态系统中存储部分的一个有趣的替代品----apache-spark--pyspark-apache-spark-是一种用于大数据工作负载的分布式开源处理系统它使用内存中缓存和优化的查询执行方式可针对任何规模的数据进行快速分析查询它提供使用-javascalapython-和-r-语言的开发-api支持跨多个工作负载重用代码批处理交互式查询实时分析机器学习和图形处理等--apache-spark是用scala编程语言编写的pyspark的发布是为了支持apache-spark和python的协作它实际上是spark的一个python-api此外pyspark帮助你在apache-spark和python编程语言中与弹性分布式数据集rdds对接这是通过利用py4j库实现的py4j是一个流行的库它集成在pyspark中允许python动态地与jvm对象对接--pyspark-1024x164pnghttpsae05alicdncomkfhbd09a67a761b449fa201263911aec493ypng---jupyter-notebook-jupyter-notebook是一个开源的web应用允许用户创建和分享包含实时代码方程式可视化和叙述性文本的文档其用途包括数据分析统计建模数据可视化机器学习等等jupyter这个词是juliapython和r的松散缩写不过现在jupyter已经可以支持许多其他的编程语言---movielens-数据集-grouplens-research从movielens网站httpsmovielensorg收集并提供了电影评分数据集总共58098个电影包含了27753444个评价和1108997个标签--数据集下载地址-httpfilesgrouplensorgdatasetsmovielensml-latestzip---下面就主要使用这几个技术来对数据集进行简单的分析并最终将分析结果保存的数据库中---准备环境-这里使用-docker-compose-部署-4-个节点的-minio-集群然后通过-nginx-进行反向代理---version-37---settings-and-configurations-that-are-common-for-all-containers-x-minio-common-minio-common-image-miniominio-command-server---console-address-9001-httpminio14data12-expose---9000---9001-environment-minio_root_user-minio-minio_root_password-minio123-healthcheck-test-cmd-curl--f-httplocalhost9000miniohealthlive-interval-30s-timeout-20s-retries-3---starts-4-docker-containers-running-minio-server-instances--using-nginx-reverse-proxy-load-balancing-you-can-access--it-through-port-9000-services-minio1--minio-common-hostname-minio1-volumes---miniodatadata1-1data1---miniodatadata1-2data2--minio2--minio-common-hostname-minio2-volumes---miniodatadata2-1data1---miniodatadata2-2data2--minio3--minio-common-hostname-minio3-volumes---miniodatadata3-1data1---miniodatadata3-2data2--minio4--minio-common-hostname-minio4-volumes---miniodatadata4-1data1---miniodatadata4-2data2--nginx-image-nginx1192-alpine-hostname-nginx-volumes---nginxconfetcnginxnginxconfro-ports---90009000---90019001-depends_on---minio1---minio2---minio3---minio4---jupyterall-spark-notebook-notebook-container_name-jupyter_notebook-image-jupyterall-spark-notebook-ports---88888888---40404040-environment---pyspark_submit_args--packages-comamazonawsaws-java-sdk11295orgapachehadoophadoop-client331comamazonawsaws-java-sdk-bundle11295orgapachehadoophadoop-aws331-pyspark-shell-volumes---workhomejovyanwork--databases-pgdb-container_name-pg_container-image-postgres-restart-always-environment-postgres_user-admin-postgres_password-admin-postgres_db-test_db-ports---54325432-pgadmin-container_name-pgadmin4_container-image-dpagepgadmin461-restart-always-environment-pgadmin_default_email-adminadmincom-pgadmin_default_password-admin-ports---505080---然后通过docker-compose-up--d启动集群-在这个-docker-compose-文件中同时化部署了postgresql数据库以及postgresql-admin-管理ui---minio-gui-通过上面的-docker-compose-命令启动了启动了4个实例的minio集群并通过nginx进行反向代理-可以通过-httplocalhost9000-来访问-minio-的控制台--imagepnghttpsae05alicdncomkfh369e1419546e449c8af79251e88defc5epng---jupyter-notebook-通过-httplocalhost8888-访问-jupyter-notebook-由于访问nupyter-dashboard首先需要一个token这里首先需要从-docker-logs-中获取该-token---docker-logs-docker-ps--grep-jupyter_notebook--awk-print-nf---imagepnghttpsae03alicdncomkfh536184483fbe48fb8f77d5d8757b2b83gpng--从-log-中可以看到-token93bc05d6549e689c3409a0ac60b58883c13236aa94245306-的内容就可以使用这个-token-来登录了--imagepnghttpsae05alicdncomkfhcb3d4fd18c0d4d44be3e0b35b564d34fbpng---postgresql-admin-通过-httplocalhost5050-来访问-postgresql-admin-dashboard-通过-add-new-server-将-postgresql-添加进来然后就可以通过-postgresql-admin-来管理数据库了--imagepnghttpsae04alicdncomkfh2a8168b1e50c42208cb3100cf14fca36ppng---imagepnghttpsae04alicdncomkfhecd346ff66d14323877385de6bb99ecbbpng----导入测试数据集-下载上面的电影评分数据集并在-minio-中创建-bucket1-bucket并将解压后的-csv-文件上传到该-bucket-中-imagepnghttpsae01alicdncomkfh1d60933a57634f1bae001fe561acbb4bnpng-总共数据大小在1gb左右---创建-jupyter-notebook-直接通过jupyterlab创建notebook-然后直接在notebook中直接编写python代码来调用spark进行数据的分析操作-imagepnghttpsae02alicdncomkfhb2558ddfcdc54965a33a5c9dd98411cbypng--比如这里调用-spark-来计算从1加到100的值-imagepnghttpsae03alicdncomkfh6c038af6480b4d489dace42ea753fbc87png--在-docker-compose的-yaml-配置中-notebook-通过-environment-属性配置了-pyspark_submit_args-参数初次运行-spark-的时候会检查并下载指定的-packages-这里主要使用了aws的-java-sdk通过amazon-s3-api-同-minio-进行通信--yaml-environment---pyspark_submit_args--packages-comamazonawsaws-java-sdk11295orgapachehadoophadoop-client331comamazonawsaws-java-sdk-bundle11295orgapachehadoophadoop-aws331-pyspark-shell---相应的spark的job可以通过-httplocalhost4040-页面查看-imagepnghttpsae05alicdncomkfh5c11c45d97784f638e016c135943e1bftpng---分析-movielens-数据集--下面就通过spark来分析movidlens的数据集--主要是从minio读取数据集中的csv文件注册成为table通过sql查询出top-100的电影然后将分析结果保存到minio以及postgresql数据库---初始化-sparksession--在读取csv文件之前首先需要创建-sparksession-的实例-spark由于spark主要是内存计算这里通过sparkdrivermemory配置参数配置两个内存否则运行过程中会产生oom的问题---from-pysparksql-import-sparksession-spark--sparksessionbuilderconfigsparkdrivermemory-5ggetorcreate----配置s3minio连接信息--python-sparksparkcontext_jsc-hadoopconfigurationsetfss3aaccesskey-minio-sparksparkcontext_jsc-hadoopconfigurationsetfss3asecretkey-minio123-sparksparkcontext_jsc-hadoopconfigurationsetfss3aendpoint-http192168099000-sparksparkcontext_jsc-hadoopconfigurationsetsparkhadoopfss3aimpl-orgapachehadoopfss3as3afilesystem-sparksparkcontext_jsc-hadoopconfigurationsetsparkhadoopfss3apathstyleaccess-true-sparksparkcontext_jsc-hadoopconfigurationsetfss3amultipartsize-104857600----读取-minio-csv-文件--python-ratings--sparkread-optionheader-true-optioninferschema-true-csvs3abucket1ratingscsv-ratingscreateorreplacetempviewratings-ratingsshow---csv中的内容则以表格的形式显示如下---------------------------------useridmovieidrating-timestamp--------------------------------1-307-351256677221--1-481-351256677456--1-1091-151256677471--1-1257-451256677460--1-1449-451256677264--1-1590-251256677236--1-1591-151256677475--1-2134-451256677464--1-2478-401256677239--1-2840-301256677500--1-2986-251256677496--1-3020-401256677260--1-3424-451256677444--1-3698-351256677243--1-3826-201256677210--1-3893-351256677486--2-170-351192913581--2-849-351192913537--2-1186-351192913611--2-1235-301192913585-------------------------------only-showing-top-20-rows---再次读取moviescsv--python-movies--sparkread-optionheader-true-optioninferschema-true-csvs3abucket1moviescsv-moviesregistertemptablemovies-moviesshow--------------------------------------------------movieid-title-genres--------------------------------------------------1-toy-story-1995adventureanimati--2-jumanji-1995adventurechildre--3grumpier-old-men--comedyromance--4waiting-to-exhalecomedydramaromance--5father-of-the-bri-comedy--6-heat-1995actioncrimethri--7-sabrina-1995-comedyromance--8-tom-and-huck-1995-adventurechildren--9-sudden-death-1995-action--10-goldeneye-1995actionadventure--11american-presidencomedydramaromance--12dracula-dead-and-comedyhorror--13-balto-1995adventureanimati--14-nixon-1995-drama--15cutthroat-island-actionadventure--16-casino-1995-crimedrama--17sense-and-sensibi-dramaromance--18-four-rooms-1995-comedy--19ace-ventura-when-comedy--20-money-train-1995actioncomedycri-------------------------------------------------only-showing-top-20-rows------计算-top-100--python-top_100_movies--sparksql-select-title-avgrating-as-avg_rating-from-movies-m-left-join-ratings-r-on-mmovieid--rmovieid-group-by-title-having-count--100-order-by-avg_rating-desc-limit-100---top_100_moviesshow--spark-processing-----------------------------------------title-avg_rating----------------------------------------planet-earth-ii-44865181711606095--planet-earth-2006-4458092485549133-shawshank-redempt-4424188001918387-band-of-brothers--4399898373983739-black-mirror-whi-4350558659217877--cosmos-4343949044585988-the-godfather-tri-4339667458432304-godfather-the-1-4332892749244713-usual-suspects-t-4291958829205532--black-mirror-4263888888888889-godfather-part-i42630353697749195-last-years-snow--4261904761904762-schindlers-list--4257501817775044-seven-samurai-sh42541157909178215-over-the-garden-w-4244031830238727-sherlock---a-stud-423943661971831--12-angry-men-1957-4237075455914338-blue-planet-ii-2-4236389684813753--rear-window-1954-4230798598634567--fight-club-1999-4230663235786717----------------------------------------only-showing-top-20-rows----保存-top-100-到-minio--python-top_100_movieswriteparquets3abucket1resultstop_100_movies--imagepnghttpsae02alicdncomkfh37b38c345b8e4eaea4059fcef2b38b860png--当运行结束后可以通过minio控制台查看对应的top-100的数据已经保存到了minio中了--image-20211029152355877pnghttpsae02alicdncomkfh5de33ed814854ae5a7e942e6307f4d28rpng---从minio读取parquet--python-sparkreadparquets3abucket1resultstop_100_moviesshow------------------------------------------title-avg_rating----------------------------------------planet-earth-ii-44865181711606095--planet-earth-2006-4458092485549133-shawshank-redempt-4424188001918387-band-of-brothers--4399898373983739-black-mirror-whi-4350558659217877--cosmos-4343949044585988-the-godfather-tri-4339667458432304-godfather-the-1-4332892749244713-usual-suspects-t-4291958829205532--black-mirror-4263888888888889-godfather-part-i42630353697749195-last-years-snow--4261904761904762-schindlers-list--4257501817775044-seven-samurai-sh42541157909178215-over-the-garden-w-4244031830238727-sherlock---a-stud-423943661971831--12-angry-men-1957-4237075455914338-blue-planet-ii-2-4236389684813753--rear-window-1954-4230798598634567--fight-club-1999-4230663235786717----------------------------------------only-showing-top-20-rows----保存数据到postgresql数据库--上面将分析的数据保存到了minio文件系统下面则将-top-100-的电影数据保存到-postgresql-数据库---安装pipy包--为了访问postgresql数据库首先需要安装相应的psycopg2-binary和sqlalchemypython包--python-pip-install--i-httpspypidoubaniocomsimple---trusted-host-pypidoubaniocom-psycopg2-binary-sqlalchemy----保存到数据库--python-import-psycopg2-import-pandas-as-pd-from-sqlalchemy-import-create_engine--top_100_df--top_100_moviestopandas---create-sqlalchemy-engine-engine--create_enginepostgresqlpsycopg2rootroot192168095432test_dbclient_encodingutf8--save-result-to-the-database-via-engine-top_100_dfto_sqltest_table-engine-indexfalse-if_existsreplace---执行完成之后就可以在pgadmin中查看结果了--imagepnghttpsae04alicdncomkfh72781a0906b349eea27a31001a9a16b3apng-----读取postgresql数据--上面是保存到数据库同样的也可以读取数据库中的表数据然后加载到spark中进行分析--python-import-psycopg2-import-pandas-as-pd-from-pysparksql-import-sparksession-from-sqlalchemy-import-create_engine--engine--create_enginepostgresqlpsycopg2adminadmin192168095432test_dbclient_encodingutf8-pdf--pdread_sqlselect-indexmd-from-test_table-engine---convert-pandas-dataframe-to-spark-dataframe-df--sparkcreatedataframepdf-printdfschema-dfshow--structtypeliststructfieldtitlestringtypetruestructfieldavg_ratingdoubletypetrue-----------------------------------------title-avg_rating----------------------------------------planet-earth-ii-44865181711606095--planet-earth-2006-4458092485549133-shawshank-redempt-4424188001918387-band-of-brothers--4399898373983739-black-mirror-whi-4350558659217877--cosmos-4343949044585988-the-godfather-tri-4339667458432304-godfather-the-1-4332892749244713-usual-suspects-t-4291958829205532--black-mirror-4263888888888889-godfather-part-i42630353697749195-last-years-snow--4261904761904762-schindlers-list--4257501817775044-seven-samurai-sh42541157909178215-over-the-garden-w-4244031830238727-sherlock---a-stud-423943661971831--12-angry-men-1957-4237075455914338-blue-planet-ii-2-4236389684813753--rear-window-1954-4230798598634567--fight-club-1999-4230663235786717----------------------------------------only-showing-top-20-rows----总结--上面显示了通过-apache-sparkpyspark-minio来分析movielens数据集得益于docker-jupyter-docker-stacks等基础工具从构建环境到使用jupyter笔记本pythonspark和pyspark开始学习和执行数据分析相当的容易并且还可以在堆栈中添加额外的容器如mysqlmongodbrabbitmqapache-kafka和apache-cassandra构建一个更为完备的分析平台>MinIO
MinIO是一个用Golang开发的基于GNU Affero General Public License v3.0开源协议的高性能对象存储服务。
MinIO兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等。它可以部署在企业的内部环境中，为机器学习、分析和应用数据工作负载建立高性能的存储基础设施。
MinIO用作云原生应用程序的主要存储解决方案，可以与Kubernetes相结合，成为Hadoop生态系统中存储部分的一个有趣的替代品。
## Apache Spark / PySpark
Apache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。
Apache Spark是用Scala编程语言编写的。PySpark的发布是为了支持Apache Spark和Python的协作，它实际上是Spark的一个Python API。此外，PySpark帮助你在Apache Spark和Python编程语言中与弹性分布式数据集（RDDs）对接。这是通过利用Py4J库实现的。Py4J是一个流行的库，它集成在PySpark中，允许python动态地与JVM对象对接。
<img loading=lazy src=https://ae05.alicdn.com/kf/Hbd09a67a761b449fa201263911aec493y.png alt=PySpark-1024x164.png>
## Jupyter Notebook
Jupyter Notebook是一个开源的WEB应用，允许用户创建和分享包含实时代码、方程式、可视化和叙述性文本的文档。其用途包括数据分析、统计建模、数据可视化、机器学习等等。Jupyter这个词是Julia、Python和R的松散缩写，不过现在Jupyter已经可以支持许多其他的编程语言。
## MovieLens 数据集
GroupLens Research从MovieLens网站（https://movielens.org）收集并提供了电影评分数据集，总共58098个电影，包含了27753444个评价和1108997个标签。
数据集下载地址： <a href=http://files.grouplens.org/datasets/movielens/ml-latest.zip>http://files.grouplens.org/datasets/movielens/ml-latest.zip</a>
下面就主要使用这几个技术来对数据集进行简单的分析，并最终将分析结果保存的数据库中。
## 准备环境
这里使用 docker compose 部署 4 个节点的 MinIO 集群，然后通过 Nginx 进行反向代理
<code>version: '3.7'
# Settings and configurations that are common for all containers
x-minio-common: &minio-common
image: minio/minio
command: server --console-address ":9001" http://minio{1...4}/data{1...2}
expose:
- "9000"
- "9001"
environment:
MINIO_ROOT_USER: minio
MINIO_ROOT_PASSWORD: minio123
healthcheck:
test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
interval: 30s
timeout: 20s
retries: 3
# starts 4 docker containers running minio server instances.
# using nginx reverse proxy, load balancing, you can access
# it through port 9000.
services:
minio1:
&lt;&lt;: *minio-common
hostname: minio1
volumes:
- ./minio/data/data1-1:/data1
- ./minio/data/data1-2:/data2
minio2:
&lt;&lt;: *minio-common
hostname: minio2
volumes:
- ./minio/data/data2-1:/data1
- ./minio/data/data2-2:/data2
minio3:
&lt;&lt;: *minio-common
hostname: minio3
volumes:
- ./minio/data/data3-1:/data1
- ./minio/data/data3-2:/data2
minio4:
&lt;&lt;: *minio-common
hostname: minio4
volumes:
- ./minio/data/data4-1:/data1
- ./minio/data/data4-2:/data2
nginx:
image: nginx:1.19.2-alpine
hostname: nginx
volumes:
- ./nginx.conf:/etc/nginx/nginx.conf:ro
ports:
- "9000:9000"
- "9001:9001"
depends_on:
- minio1
- minio2
- minio3
- minio4
# jupyter/all-spark-notebook
notebook:
container_name: jupyter_notebook
image: jupyter/all-spark-notebook
ports:
- 8888:8888
- 4040:4040
environment:
- PYSPARK_SUBMIT_ARGS=--packages com.amazonaws:aws-java-sdk:1.12.95,org.apache.hadoop:hadoop-client:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.95,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell
volumes:
- ./work:/home/jovyan/work
# Databases
pgdb:
container_name: pg_container
image: postgres
restart: always
environment:
POSTGRES_USER: admin
POSTGRES_PASSWORD: admin
POSTGRES_DB: test_db
ports:
- "5432:5432"
pgadmin:
container_name: pgadmin4_container
image: dpage/pgadmin4:6.1
restart: always
environment:
PGADMIN_DEFAULT_EMAIL: admin@admin.com
PGADMIN_DEFAULT_PASSWORD: admin
ports:
- "5050:80"</code>
然后通过<code>docker compose up -d</code>启动集群. 在这个 docker compose 文件中同时化部署了PostgreSQL数据库以及PostgreSQL Admin 管理UI。
### MinIO GUI
通过上面的 <code>docker compose</code> 命令启动了启动了4个实例的MinIO集群，并通过Nginx进行反向代理, 可以通过 http://localhost:9000 来访问 MinIO 的控制台。
<img loading=lazy src=https://ae05.alicdn.com/kf/H369e1419546e449c8af79251e88defc5e.png alt=image.png>
### Jupyter Notebook
通过 http://localhost:8888 访问 Jupyter Notebook。
由于访问Nupyter Dashboard首先需要一个token，这里首先需要从 docker logs 中获取该 token
<code>docker logs $(docker ps | grep jupyter_notebook | awk '{print $NF}')</code>
<img loading=lazy src=https://ae03.alicdn.com/kf/H536184483fbe48fb8f77d5d8757b2b83G.png alt=image.png>
从 log 中可以看到 <code>?token=93bc05d6549e689c3409a0ac60b58883c13236aa94245306</code> 的内容，就可以使用这个 token 来登录了:
<img loading=lazy src=https://ae05.alicdn.com/kf/Hcb3d4fd18c0d4d44be3e0b35b564d34fb.png alt=image.png>
### PostgreSQL Admin
通过 http://localhost:5050 来访问 PostgreSQL admin dashboard, 通过 &ldquo;Add New Server&rdquo; 将 PostgreSQL 添加进来，然后就可以通过 PostgreSQL Admin 来管理数据库了。
<img loading=lazy src=https://ae04.alicdn.com/kf/H2a8168b1e50c42208cb3100cf14fca36P.png alt=image.png>
<img loading=lazy src=https://ae04.alicdn.com/kf/Hecd346ff66d14323877385de6bb99ecbb.png alt=image.png>
## 导入测试数据集
下载上面的电影评分数据集，并在 MinIO 中创建 <code>bucket1</code> Bucket，并将解压后的 csv 文件上传到该 bucket 中。
<img loading=lazy src=https://ae01.alicdn.com/kf/H1d60933a57634f1bae001fe561acbb4bn.png alt=image.png>
总共数据大小在1GB左右。
## 创建 Jupyter Notebook
直接通过JupyterLab创建Notebook, 然后直接在notebook中直接编写python代码来调用Spark进行数据的分析操作。
<img loading=lazy src=https://ae02.alicdn.com/kf/Hb2558ddfcdc54965a33a5c9dd98411cbY.png alt=image.png>
比如这里调用 Spark 来计算从1加到100的值：
<img loading=lazy src=https://ae03.alicdn.com/kf/H6c038af6480b4d489dace42ea753fbc87.png alt=image.png>
在 docker compose的 <code>yaml</code> 配置中， <code>notebook</code> 通过 <code>environment</code> 属性配置了 <code>PYSPARK_SUBMIT_ARGS</code> 参数，初次运行 Spark 的时候，会检查并下载指定的 packages。
这里主要使用了AWS的 JAVA SDK，通过Amazon S3 API 同 MinIO 进行通信。
<code>yaml
environment:
- PYSPARK_SUBMIT_ARGS=--packages com.amazonaws:aws-java-sdk:1.12.95,org.apache.hadoop:hadoop-client:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.95,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell</code>
相应的Spark的Job可以通过 http://localhost:4040 页面查看:
<img loading=lazy src=https://ae05.alicdn.com/kf/H5c11c45d97784f638e016c135943e1bfT.png alt=image.png>
## 分析 MovieLens 数据集
下面就通过Spark来分析MovidLens的数据集。
主要是从MinIO读取数据集中的CSV文件，注册成为table，通过SQL查询出top 100的电影，然后将分析结果保存到MinIO以及PostgreSQL数据库。
### 初始化 SparkSession
在读取CSV文件之前，首先需要创建 <code>SparkSession</code> 的实例 <code>spark</code>，由于Spark主要是内存计算，这里通过<code>spark.driver.memory</code>配置参数配置两个内存，否则运行过程中会产生OOM的问题。
<code>from pyspark.sql import SparkSession
spark = SparkSession.builder.config("spark.driver.memory", "5g").getOrCreate()</code>
### 配置S3/MinIO连接信息
<code>python
spark.sparkContext._jsc\
.hadoopConfiguration().set("fs.s3a.access.key", "minio")
spark.sparkContext._jsc\
.hadoopConfiguration().set("fs.s3a.secret.key", "minio123")
spark.sparkContext._jsc\
.hadoopConfiguration().set("fs.s3a.endpoint", "http://192.168.0.9:9000")
spark.sparkContext._jsc\
.hadoopConfiguration().set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
spark.sparkContext._jsc\
.hadoopConfiguration().set("spark.hadoop.fs.s3a.path.style.access", "true")
spark.sparkContext._jsc\
.hadoopConfiguration().set("fs.s3a.multipart.size", "104857600")</code>
### 读取 MinIO CSV 文件
<code>python
ratings = spark.read\
.option("header", "true")\
.option("inferSchema", "true")\
.csv("s3a://bucket1/ratings.csv")
ratings.createOrReplaceTempView("ratings")
ratings.show()</code>
CSV中的内容则以表格的形式显示如下：
<code>+------+-------+------+----------+
|userId|movieId|rating| timestamp|
+------+-------+------+----------+
| 1| 307| 3.5|1256677221|
| 1| 481| 3.5|1256677456|
| 1| 1091| 1.5|1256677471|
| 1| 1257| 4.5|1256677460|
| 1| 1449| 4.5|1256677264|
| 1| 1590| 2.5|1256677236|
| 1| 1591| 1.5|1256677475|
| 1| 2134| 4.5|1256677464|
| 1| 2478| 4.0|1256677239|
| 1| 2840| 3.0|1256677500|
| 1| 2986| 2.5|1256677496|
| 1| 3020| 4.0|1256677260|
| 1| 3424| 4.5|1256677444|
| 1| 3698| 3.5|1256677243|
| 1| 3826| 2.0|1256677210|
| 1| 3893| 3.5|1256677486|
| 2| 170| 3.5|1192913581|
| 2| 849| 3.5|1192913537|
| 2| 1186| 3.5|1192913611|
| 2| 1235| 3.0|1192913585|
+------+-------+------+----------+
only showing top 20 rows</code>
再次读取<code>movies</code>CSV
<code>python
movies = spark.read\
.option("header", "true")\
.option("inferSchema", "true")\
.csv("s3a://bucket1/movies.csv")
movies.registerTempTable("movies")
movies.show()
+-------+--------------------+--------------------+
|movieId| title| genres|
+-------+--------------------+--------------------+
| 1| Toy Story (1995)|Adventure|Animati...|
| 2| Jumanji (1995)|Adventure|Childre...|
| 3|Grumpier Old Men ...| Comedy|Romance|
| 4|Waiting to Exhale...|Comedy|Drama|Romance|
| 5|Father of the Bri...| Comedy|
| 6| Heat (1995)|Action|Crime|Thri...|
| 7| Sabrina (1995)| Comedy|Romance|
| 8| Tom and Huck (1995)| Adventure|Children|
| 9| Sudden Death (1995)| Action|
| 10| GoldenEye (1995)|Action|Adventure|...|
| 11|American Presiden...|Comedy|Drama|Romance|
| 12|Dracula: Dead and...| Comedy|Horror|
| 13| Balto (1995)|Adventure|Animati...|
| 14| Nixon (1995)| Drama|
| 15|Cutthroat Island ...|Action|Adventure|...|
| 16| Casino (1995)| Crime|Drama|
| 17|Sense and Sensibi...| Drama|Romance|
| 18| Four Rooms (1995)| Comedy|
| 19|Ace Ventura: When...| Comedy|
| 20| Money Train (1995)|Action|Comedy|Cri...|
+-------+--------------------+--------------------+
only showing top 20 rows</code>
### 计算 top 100
<code>python
top_100_movies = spark.sql("""
SELECT title, AVG(rating) as avg_rating
FROM movies m
LEFT JOIN ratings r ON m.movieId = r.movieID
GROUP BY title
HAVING COUNT(*) > 100
ORDER BY avg_rating DESC
LIMIT 100
""")
top_100_movies.show()
# Spark Processing
+--------------------+------------------+
| title| avg_rating|
+--------------------+------------------+
|Planet Earth II (...|4.4865181711606095|
| Planet Earth (2006)| 4.458092485549133|
|Shawshank Redempt...| 4.424188001918387|
|Band of Brothers ...| 4.399898373983739|
|Black Mirror: Whi...| 4.350558659217877|
| Cosmos| 4.343949044585988|
|The Godfather Tri...| 4.339667458432304|
|Godfather, The (1...| 4.332892749244713|
|Usual Suspects, T...| 4.291958829205532|
| Black Mirror| 4.263888888888889|
|Godfather: Part I...|4.2630353697749195|
|Last Year's Snow ...| 4.261904761904762|
|Schindler's List ...| 4.257501817775044|
|Seven Samurai (Sh...|4.2541157909178215|
|Over the Garden W...| 4.244031830238727|
|Sherlock - A Stud...| 4.23943661971831|
| 12 Angry Men (1957)| 4.237075455914338|
|Blue Planet II (2...| 4.236389684813753|
| Rear Window (1954)| 4.230798598634567|
| Fight Club (1999)| 4.230663235786717|
+--------------------+------------------+
only showing top 20 rows</code>
### 保存 top 100 到 MinIO
<code>python
top_100_movies.write.parquet("s3a://bucket1/results/top_100_movies")</code>
<img loading=lazy src=https://ae02.alicdn.com/kf/H37b38c345b8e4eaea4059fcef2b38b860.png alt=image.png>
当运行结束后，可以通过MinIO控制台查看，对应的top 100的数据，已经保存到了MinIO中了。
<img loading=lazy src=https://ae02.alicdn.com/kf/H5de33ed814854ae5a7e942e6307f4d28r.png alt=image-20211029152355877.png>
### 从MinIO读取Parquet
<code>python
spark.read.parquet("s3a://bucket1/results/top_100_movies").show()
+--------------------+------------------+
| title| avg_rating|
+--------------------+------------------+
|Planet Earth II (...|4.4865181711606095|
| Planet Earth (2006)| 4.458092485549133|
|Shawshank Redempt...| 4.424188001918387|
|Band of Brothers ...| 4.399898373983739|
|Black Mirror: Whi...| 4.350558659217877|
| Cosmos| 4.343949044585988|
|The Godfather Tri...| 4.339667458432304|
|Godfather, The (1...| 4.332892749244713|
|Usual Suspects, T...| 4.291958829205532|
| Black Mirror| 4.263888888888889|
|Godfather: Part I...|4.2630353697749195|
|Last Year's Snow ...| 4.261904761904762|
|Schindler's List ...| 4.257501817775044|
|Seven Samurai (Sh...|4.2541157909178215|
|Over the Garden W...| 4.244031830238727|
|Sherlock - A Stud...| 4.23943661971831|
| 12 Angry Men (1957)| 4.237075455914338|
|Blue Planet II (2...| 4.236389684813753|
| Rear Window (1954)| 4.230798598634567|
| Fight Club (1999)| 4.230663235786717|
+--------------------+------------------+
only showing top 20 rows</code>
### 保存数据到PostgreSQL数据库
上面将分析的数据保存到了MinIO文件系统，下面则将 top 100 的电影数据保存到 PostgreSQL 数据库。
#### 安装pipy包
为了访问PostgreSQL数据库，首先需要安装相应的<code>psycopg2-binary</code>和<code>sqlalchemy</code>python包。
<code>python
pip install -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com psycopg2-binary sqlalchemy</code>
#### 保存到数据库
<code>python
import psycopg2
import pandas as pd
from sqlalchemy import create_engine
top_100_df = top_100_movies.toPandas()
# Create SQLAlchemy engine
engine = create_engine("postgresql+psycopg2://root:root@192.168.0.9:5432/test_db?client_encoding=utf8")
# Save result to the database via engine
top_100_df.to_sql('test_table', engine, index=False, if_exists='replace')</code>
执行完成之后，就可以在PGAdmin中查看结果了。
<img loading=lazy src=https://ae04.alicdn.com/kf/H72781a0906b349eea27a31001a9a16b3A.png alt=image.png>
#### 读取PostgreSQL数据
上面是保存到数据库，同样的也可以读取数据库中的表数据，然后加载到Spark中进行分析
<code>python
import psycopg2
import pandas as pd
from pyspark.sql import SparkSession
from sqlalchemy import create_engine
engine = create_engine("postgresql+psycopg2://admin:admin@192.168.0.9:5432/test_db?client_encoding=utf8")
pdf = pd.read_sql('select index.md from test_table', engine)
# Convert Pandas dataframe to spark DataFrame
df = spark.createDataFrame(pdf)
print(df.schema)
df.show()
StructType(List(StructField(title,StringType,true),StructField(avg_rating,DoubleType,true)))
+--------------------+------------------+
| title| avg_rating|
+--------------------+------------------+
|Planet Earth II (...|4.4865181711606095|
| Planet Earth (2006)| 4.458092485549133|
|Shawshank Redempt...| 4.424188001918387|
|Band of Brothers ...| 4.399898373983739|
|Black Mirror: Whi...| 4.350558659217877|
| Cosmos| 4.343949044585988|
|The Godfather Tri...| 4.339667458432304|
|Godfather, The (1...| 4.332892749244713|
|Usual Suspects, T...| 4.291958829205532|
| Black Mirror| 4.263888888888889|
|Godfather: Part I...|4.2630353697749195|
|Last Year's Snow ...| 4.261904761904762|
|Schindler's List ...| 4.257501817775044|
|Seven Samurai (Sh...|4.2541157909178215|
|Over the Garden W...| 4.244031830238727|
|Sherlock - A Stud...| 4.23943661971831|
| 12 Angry Men (1957)| 4.237075455914338|
|Blue Planet II (2...| 4.236389684813753|
| Rear Window (1954)| 4.230798598634567|
| Fight Club (1999)| 4.230663235786717|
+--------------------+------------------+
only showing top 20 rows</code>
## 总结
上面显示了通过 Apache Spark/PySpark, MinIO来分析MovieLens数据集，得益于Docker, Jupyter Docker Stacks等基础工具，从构建环境，到使用Jupyter笔记本、Python、Spark和PySpark开始学习和执行数据分析相当的容易，并且还可以在堆栈中添加额外的容器，如MySQL、MongoDB、RabbitMQ、Apache Kafka和Apache Cassandra。构建一个更为完备的分析平台。</h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://technotes.guru/tags/publish/>publish</a></li><li><a href=https://technotes.guru/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/>大数据</a></li><li><a href=https://technotes.guru/tags/spark/>Spark</a></li><li><a href=https://technotes.guru/tags/pyspark/>PySpark</a></li></ul><nav class=paginav><a class=next href=https://technotes.guru/posts/2021/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E7%AC%94%E8%AE%B0/><span class=title>Next Page »</span><br><span>数据中台笔记</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://technotes.guru/>TechNotes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>